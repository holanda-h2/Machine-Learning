{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    },
    "colab": {
      "name": "Bayesian_Inference.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "UoD0vVaauQFx",
        "uEEyb-IWuQF2"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/holanda-h2/Machine-Learning/blob/main/bayes/Bayesian_Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GgkDDa7uQFp"
      },
      "source": [
        "## Our Mission ##\n",
        "\n",
        "Spam detection is one of the major applications of Machine Learning in the interwebs today. Pretty much all of the major email service providers have spam detection systems built in and automatically classify such mail as 'Junk Mail'. \n",
        "\n",
        "A detecção de spam é uma das principais aplicações do aprendizado de máquina nas interwebs hoje. Quase todos os principais provedores de serviço de e-mail possuem sistemas de detecção de spam embutidos e classificam automaticamente esse tipo de e-mail como 'Lixo Eletrônico'.\n",
        "\n",
        "In this mission we will be using the Naive Bayes algorithm to create a model that can classify [dataset](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) SMS messages as spam or not spam, based on the training we give to the model. It is important to have some level of intuition as to what a spammy text message might look like. Usually they have words like 'free', 'win', 'winner', 'cash', 'prize' and the like in them as these texts are designed to catch your eye and in some sense tempt you to open them. Also, spam messages tend to have words written in all capitals and also tend to use a lot of exclamation marks. To the recipient, it is usually pretty straightforward to identify a spam text and our objective here is to train a model to do that for us!\n",
        "\n",
        "Nesta missão, usaremos o algoritmo Naive Bayes para criar um modelo que possa classificar [dataset] (https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) mensagens SMS como spam ou não spam, com base no treinamento que damos ao modelo. É importante ter algum nível de intuição sobre a aparência de uma mensagem de texto com spam. Normalmente, eles contêm palavras como 'grátis', 'ganhar', 'vencedor', 'dinheiro', 'prêmio' e semelhantes, pois esses textos são projetados para chamar sua atenção e, de alguma forma, tentá-lo a abri-los. Além disso, as mensagens de spam tendem a ter palavras escritas em maiúsculas e também tendem a usar muitos pontos de exclamação. Para o destinatário, geralmente é muito simples identificar um texto de spam e nosso objetivo aqui é treinar um modelo para fazer isso por nós!\n",
        "\n",
        "Being able to identify spam messages is a binary classification problem as messages are classified as either 'Spam' or 'Not Spam' and nothing else. Also, this is a supervised learning problem, as we will be feeding a labelled dataset into the model, that it can learn from, to make future predictions. \n",
        "\n",
        "Ser capaz de identificar mensagens de spam é um problema de classificação binária, pois as mensagens são classificadas como 'Spam' ou 'Não é spam' e nada mais. Além disso, este é um problema de aprendizado supervisionado, já que alimentaremos um conjunto de dados rotulado no modelo, com o qual ele pode aprender, para fazer previsões futuras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoD0vVaauQFx"
      },
      "source": [
        "### Step 0: Introduction to the Naive Bayes Theorem ###\n",
        "\n",
        "Bayes theorem is one of the earliest probabilistic inference algorithms developed by Reverend Bayes (which he used to try and infer the existence of God no less) and still performs extremely well for certain use cases. \n",
        "\n",
        "O teorema de Bayes é um dos primeiros algoritmos de inferência probabilística desenvolvidos pelo Reverendo Bayes (que ele usou para tentar inferir a existência de Deus nada menos) e ainda tem um desempenho extremamente bom para certos casos de uso.\n",
        "\n",
        "It's best to understand this theorem using an example. Let's say you are a member of the Secret Service and you have been deployed to protect the Democratic presidential nominee during one of his/her campaign speeches. Being a public event that is open to all, your job is not easy and you have to be on the constant lookout for threats. So one place to start is to put a certain threat-factor for each person. So based on the features of an individual, like the age, sex, and other smaller factors like is the person carrying a bag?, does the person look nervous? etc. you can make a judgement call as to if that person is viable threat. \n",
        "\n",
        "É melhor entender esse teorema usando um exemplo. Digamos que você seja um membro do Serviço Secreto e tenha sido designado para proteger o candidato democrata à presidência durante um de seus discursos de campanha. Por ser um evento público aberto a todos, seu trabalho não é fácil e você tem que estar sempre alerta para as ameaças. Portanto, um lugar para começar é colocar um certo fator de ameaça para cada pessoa. Portanto, com base nas características de um indivíduo, como idade, sexo e outros fatores menores, como a pessoa carregando uma bolsa ?, a pessoa parece nervosa? etc., você pode avaliar se essa pessoa é uma ameaça viável.\n",
        "\n",
        "If an individual ticks all the boxes up to a level where it crosses a threshold of doubt in your mind, you can take action and remove that person from the vicinity. The Bayes theorem works in the same way as we are computing the probability of an event(a person being a threat) based on the probabilities of certain related events(age, sex, presence of bag or not, nervousness etc. of the person). \n",
        "\n",
        "Se um indivíduo marcar todas as caixas até um nível em que cruze o limiar da dúvida em sua mente, você pode agir e remover essa pessoa da vizinhança. O teorema de Bayes funciona da mesma forma que computamos a probabilidade de um evento (uma pessoa ser uma ameaça) com base nas probabilidades de certos eventos relacionados (idade, sexo, presença de bolsa ou não, nervosismo etc. da pessoa).\n",
        "\n",
        "One thing to consider is the independence of these features amongst each other. For example if a child looks nervous at the event then the likelihood of that person being a threat is not as much as say if it was a grown man who was nervous. To break this down a bit further, here there are two features we are considering, age AND nervousness. Say we look at these features individually, we could design a model that flags ALL persons that are nervous as potential threats. However, it is likely that we will have a lot of false positives as there is a strong chance that minors present at the event will be nervous. Hence by considering the age of a person along with the 'nervousness' feature we would definitely get a more accurate result as to who are potential threats and who aren't. \n",
        "\n",
        "Uma coisa a se considerar é a independência desses recursos entre si. Por exemplo, se uma criança parece nervosa com o evento, a probabilidade de essa pessoa ser uma ameaça não é tanto quanto se fosse um homem adulto que estava nervoso. Para detalhar um pouco mais, aqui estão duas características que estamos considerando, idade E nervosismo. Digamos que olhemos para esses recursos individualmente, poderíamos projetar um modelo que sinaliza TODAS as pessoas que estão nervosas como ameaças em potencial. No entanto, é provável que tenhamos muitos falsos positivos, pois há uma grande chance de que os menores presentes no evento fiquem nervosos. Conseqüentemente, considerando a idade de uma pessoa junto com o recurso de 'nervosismo', nós definitivamente obteríamos um resultado mais preciso sobre quem são ameaças potenciais e quem não são.\n",
        "\n",
        "This is the 'Naive' bit of the theorem where it considers each feature to be independent of each other which may not always be the case and hence that can affect the final judgement.\n",
        "\n",
        "Este é o bit 'ingênuo' do teorema, em que considera cada característica independente umas das outras, o que pode nem sempre ser o caso e, portanto, pode afetar o julgamento final.\n",
        "\n",
        "In short, the Bayes theorem calculates the probability of a certain event happening(in our case, a message being  spam) based on the joint probabilistic distributions of certain other events(in our case, a message being classified as spam). We will dive into the workings of the Bayes theorem later in the mission, but first, let us understand the data we are going to work with.\n",
        "\n",
        "Resumindo, o teorema de Bayes calcula a probabilidade de um determinado evento acontecer (no nosso caso, uma mensagem sendo spam) com base nas distribuições probabilísticas conjuntas de certos outros eventos (no nosso caso, uma mensagem sendo classificada como spam). Vamos mergulhar no funcionamento do teorema de Bayes mais tarde na missão, mas primeiro, vamos entender os dados com os quais vamos trabalhar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3LrNx1iuQFy"
      },
      "source": [
        "### Step 1.1: Understanding our dataset ### \n",
        "\n",
        "\n",
        "We will be using a [dataset](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) from the UCI Machine Learning repository which has a very good collection of datasets for experimental research purposes. The direct data link is [here](https://archive.ics.uci.edu/ml/machine-learning-databases/00228/).\n",
        "\n",
        "Estaremos usando um [conjunto de dados] (https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) do repositório de aprendizado de máquina UCI que tem uma coleção muito boa de conjuntos de dados para fins de pesquisa experimental. O link de dados direto é [aqui] (https://archive.ics.uci.edu/ml/machine-learning-databases/00228/).\n",
        "\n",
        " ** Here's a preview of the data: ** \n",
        "\n",
        "<img src=\"images/dqnb.png\" height=\"1242\" width=\"1242\">\n",
        "\n",
        "The columns in the data set are currently not named and as you can see, there are 2 columns. \n",
        "\n",
        "As colunas no conjunto de dados atualmente não são nomeadas e, como você pode ver, existem 2 colunas.\n",
        "\n",
        "The first column takes two values, 'ham' which signifies that the message is not spam, and 'spam' which signifies that the message is spam. \n",
        "\n",
        "A primeira coluna tem dois valores, 'ham', que significa que a mensagem não é spam, e 'spam', que significa que a mensagem é spam.\n",
        "\n",
        "The second column is the text content of the SMS message that is being classified.\n",
        "\n",
        "A segunda coluna é o conteúdo de texto da mensagem SMS que está sendo classificada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "eMz-XmM4uQFz"
      },
      "source": [
        ">** Instructions: **\n",
        "* Import the dataset into a pandas dataframe using the read_table method. Because this is a tab separated dataset we will be using '\\t'\n",
        " as the value for the 'sep' argument which specifies this format. \n",
        "* Importe o conjunto de dados para um dataframe do pandas usando o método read_table. Como este é um conjunto de dados separado por tabulações, usaremos '\\t'\n",
        "  como o valor para o argumento 'sep' que especifica este formato.\n",
        "* Also, rename the column names by specifying a list ['label, 'sms_message'] to the 'names' argument of read_table().\n",
        "* Além disso, renomeie os nomes das colunas especificando uma lista ['label,'sms_message'] para o argumento 'names' de read_table().\n",
        "* Print the first five values of the dataframe with the new column names.\n",
        "* Imprima os primeiros cinco valores do dataframe com os novos nomes das colunas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "LiX6xkGpuQFz",
        "outputId": "34abfefa-08cb-4702-e68c-298de58c9e76"
      },
      "source": [
        "'''\n",
        "Solution\n",
        "'''\n",
        "import pandas as pd\n",
        "# Dataset from - https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\n",
        "df = pd.read_table('/content/SMSSpamCollection',\n",
        "                   sep='\\t', \n",
        "                   header=None, \n",
        "                   names=['label', 'sms_message'])\n",
        "\n",
        "# Output printing out first 5 columns\n",
        "df.head()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>sms_message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                        sms_message\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  U dun say so early hor... U c already then say...\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyIYqg7kuQF0"
      },
      "source": [
        "### Step 1.2: Data Preprocessing ###\n",
        "\n",
        "Now that we have a basic understanding of what our dataset looks like, lets convert our labels to binary variables, 0 to represent 'ham'(i.e. not spam) and 1 to represent 'spam' for ease of computation. \n",
        "\n",
        "Agora que temos uma compreensão básica da aparência de nosso conjunto de dados, vamos converter nossos rótulos em variáveis binárias, 0 para representar 'ham' (ou seja, não é spam) e 1 para representar 'spam' para facilitar o cálculo.\n",
        "\n",
        "You might be wondering why do we need to do this step? The answer to this lies in how scikit-learn handles inputs. Scikit-learn only deals with numerical values and hence if we were to leave our label values as strings, scikit-learn would do the conversion internally(more specifically, the string labels will be cast to unknown float values). \n",
        "\n",
        "Você pode estar se perguntando por que precisamos realizar esta etapa? A resposta para isso está em como o scikit-learn trata as entradas. O Scikit-learn lida apenas com valores numéricos e, portanto, se deixarmos nossos valores de rótulo como strings, o scikit-learn faria a conversão internamente (mais especificamente, os rótulos de string serão convertidos em valores flutuantes desconhecidos).\n",
        "\n",
        "Our model would still be able to make predictions if we left our labels as strings but we could have issues later when calculating performance metrics, for example when calculating our precision and recall scores. Hence, to avoid unexpected 'gotchas' later, it is good practice to have our categorical values be fed into our model as integers.\n",
        "\n",
        "Nosso modelo ainda seria capaz de fazer previsões se deixássemos nossos rótulos como strings, mas poderíamos ter problemas mais tarde ao calcular as métricas de desempenho, por exemplo, ao calcular nossas pontuações de precisão e recall. Portanto, para evitar 'pegadinhas' inesperadas posteriormente, é uma boa prática alimentar nossos valores categóricos em nosso modelo como inteiros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "aWtypzYnuQF1"
      },
      "source": [
        ">**Instructions: **\n",
        "* Convert the values in the 'label' column to numerical values using map method as follows:\n",
        "{'ham':0, 'spam':1} This maps the 'ham' value to 0 and the 'spam' value to 1.\n",
        "* Converta os valores na coluna 'rótulo' em valores numéricos usando o método de mapa da seguinte forma:\n",
        "{'ham': 0, 'spam': 1} Isso mapeia o valor 'ham' para 0 e o valor 'spam' para 1.\n",
        "* Also, to get an idea of the size of the dataset we are dealing with, print out number of rows and columns using \n",
        "'shape'.\n",
        "* Além disso, para ter uma ideia do tamanho do conjunto de dados com o qual estamos lidando, imprima o número de linhas e colunas usando\n",
        "'shape'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "UF6tQqPDuQF2",
        "outputId": "0a583ea4-6649-4513-98bb-d2dc75ddac8e"
      },
      "source": [
        "'''\n",
        "Solution\n",
        "'''\n",
        "df['label'] = df.label.map({'ham':0, 'spam':1})\n",
        "print(df.shape)\n",
        "df.head() # returns (rows, columns)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5572, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>sms_message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label                                        sms_message\n",
              "0      0  Go until jurong point, crazy.. Available only ...\n",
              "1      0                      Ok lar... Joking wif u oni...\n",
              "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3      0  U dun say so early hor... U c already then say...\n",
              "4      0  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEEyb-IWuQF2"
      },
      "source": [
        "### Step 2.1: Bag of words ###\n",
        "\n",
        "What we have here in our data set is a large collection of text data (5,572 rows of data). Most ML algorithms rely on numerical data to be fed into them as input, and email/sms messages are usually text heavy. \n",
        "\n",
        "O que temos aqui em nosso conjunto de dados é uma grande coleção de dados de texto (5.572 linhas de dados). A maioria dos algoritmos de ML depende de dados numéricos para serem alimentados como entrada, e as mensagens de e-mail / sms geralmente têm muito texto.\n",
        "\n",
        "Here we'd like to introduce the Bag of Words(BoW) concept which is a term used to specify the problems that have a 'bag of words' or a collection of text data that needs to be worked with. The basic idea of BoW is to take a piece of text and count the frequency of the words in that text. It is important to note that the BoW concept treats each word individually and the order in which the words occur does not matter. \n",
        "\n",
        "Aqui, gostaríamos de apresentar o conceito de Bag of Words (BoW), que é um termo usado para especificar os problemas que têm uma 'bolsa de palavras' ou uma coleção de dados de texto que precisam ser trabalhados. A ideia básica do BoW é pegar um pedaço de texto e contar a frequência das palavras nesse texto. É importante notar que o conceito BoW trata cada palavra individualmente e a ordem em que as palavras ocorrem não importa.\n",
        "\n",
        "Using a process which we will go through now, we can covert a collection of documents to a matrix, with each document being a row and each word(token) being the column, and the corresponding (row,column) values being the frequency of occurrence of each word or token in that document.\n",
        "\n",
        "Usando um processo pelo qual passaremos agora, podemos converter uma coleção de documentos em uma matriz, com cada documento sendo uma linha e cada palavra (token) sendo a coluna, e os valores correspondentes (linha, coluna) sendo a frequência de ocorrência de cada palavra ou token naquele documento.\n",
        "\n",
        "For example: \n",
        "\n",
        "Lets say we have 4 documents as follows:\n",
        "Digamos que temos 4 documentos da seguinte forma:\n",
        "\n",
        "`['Hello, how are you!',\n",
        "'Win money, win from home.',\n",
        "'Call me now',\n",
        "'Hello, Call you tomorrow?']`\n",
        "\n",
        "Our objective here is to convert this set of text to a frequency distribution matrix, as follows:\n",
        "\n",
        "Nosso objetivo aqui é converter este conjunto de texto em uma matriz de distribuição de frequência, da seguinte maneira:\n",
        "\n",
        "<img src=\"images/countvectorizer.png\" height=\"542\" width=\"542\">\n",
        "\n",
        "Here as we can see, the documents are numbered in the rows, and each word is a column name, with the corresponding value being the frequency of that word in the document.\n",
        "\n",
        "Aqui, como podemos ver, os documentos são numerados nas linhas, e cada palavra é um nome de coluna, com o valor correspondente sendo a frequência dessa palavra no documento.\n",
        "\n",
        "Lets break this down and see how we can do this conversion using a small set of documents.\n",
        "\n",
        "Vamos analisar isso e ver como podemos fazer essa conversão usando um pequeno conjunto de documentos.\n",
        "\n",
        "To handle this, we will be using sklearns \n",
        "[count vectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) method which does the following:\n",
        "\n",
        "* It tokenizes the string(separates the string into individual words) and gives an integer ID to each token.\n",
        "* Ele simboliza a string (separa a string em palavras individuais) e fornece um ID inteiro para cada token.\n",
        "* It counts the occurrence of each of those tokens.\n",
        "* Conta a ocorrência de cada um desses tokens.\n",
        "\n",
        "** Please Note: ** \n",
        "\n",
        "* The CountVectorizer method automatically converts all tokenized words to their lower case form so that it does not treat words like 'He' and 'he' differently. It does this using the `lowercase` parameter which is by default set to `True`.\n",
        "\n",
        "* O método CountVectorizer converte automaticamente todas as palavras tokenizadas em minúsculas para que não trate palavras como 'Ele' e 'ele' de forma diferente. Ele faz isso usando o parâmetro `lowercase` que por padrão é definido como` True`.\n",
        "\n",
        "* It also ignores all punctuation so that words followed by a punctuation mark (for example: 'hello!') are not treated differently than the same words not prefixed or suffixed by a punctuation mark (for example: 'hello'). It does this using the `token_pattern` parameter which has a default regular expression which selects tokens of 2 or more alphanumeric characters.\n",
        "\n",
        "* Ele também ignora toda a pontuação para que as palavras seguidas por um sinal de pontuação (por exemplo: 'olá!') Não sejam tratadas de forma diferente das mesmas palavras sem prefixo ou sufixo de um sinal de pontuação (por exemplo: 'olá'). Ele faz isso usando o parâmetro `token_pattern`, que possui uma expressão regular padrão que seleciona tokens de 2 ou mais caracteres alfanuméricos.\n",
        "\n",
        "* The third parameter to take note of is the `stop_words` parameter. Stop words refer to the most commonly used words in a language. They include words like 'am', 'an', 'and', 'the' etc. By setting this parameter value to `english`, CountVectorizer will automatically ignore all words(from our input text) that are found in the built in list of english stop words in scikit-learn. This is extremely helpful as stop words can skew our calculations when we are trying to find certain key words that are indicative of spam.\n",
        "\n",
        "* O terceiro parâmetro a ser observado é o parâmetro `stop_words`. Palavras irrelevantes referem-se às palavras mais comumente usadas em um idioma. Eles incluem palavras como 'sou', 'um', 'e', 'o' etc. Ao definir este valor de parâmetro para `inglês`, CountVectorizer irá automaticamente ignorar todas as palavras (de nosso texto de entrada) que são encontradas no embutido lista de palavras de interrupção em inglês no scikit-learn. Isso é extremamente útil, pois as palavras irrelevantes podem distorcer nossos cálculos quando tentamos encontrar certas palavras-chave indicativas de spam.\n",
        "\n",
        "We will dive into the application of each of these into our model in a later step, but for now it is important to be aware of such preprocessing techniques available to us when dealing with textual data.\n",
        "\n",
        "Vamos mergulhar na aplicação de cada um deles em nosso modelo em uma etapa posterior, mas por enquanto é importante estar ciente de tais técnicas de pré-processamento disponíveis para nós quando lidamos com dados textuais."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrdnV9KsuQF3"
      },
      "source": [
        "### Step 2.2: Implementing Bag of Words from scratch ###\n",
        "\n",
        "Before we dive into scikit-learn's Bag of Words(BoW) library to do the dirty work for us, let's implement it ourselves first so that we can understand what's happening behind the scenes. \n",
        "\n",
        "Antes de mergulharmos na biblioteca Bag of Words (BoW) do scikit-learn para fazer o trabalho sujo para nós, vamos implementá-la primeiro para que possamos entender o que está acontecendo nos bastidores.\n",
        "\n",
        "** Step 1: Convert all strings to their lower case form. **\n",
        "\n",
        "Let's say we have a document set:\n",
        "\n",
        "```\n",
        "documents = ['Hello, how are you!',\n",
        "             'Win money, win from home.',\n",
        "             'Call me now.',\n",
        "             'Hello, Call hello you tomorrow?']\n",
        "```\n",
        ">>** Instructions: **\n",
        "* Convert all the strings in the documents set to their lower case. Save them into a list called 'lower_case_documents'. You can convert strings to their lower case in python by using the lower() method.\n",
        "* Converta todas as strings nos documentos definidos para minúsculas. Salve-os em uma lista chamada 'lower_case_documents'. Você pode converter strings para minúsculas em python usando o método lower ()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9L2cREKuQF4",
        "outputId": "eb098226-79af-4c45-90f8-2f6d09d3a828"
      },
      "source": [
        "'''\n",
        "Solution:\n",
        "'''\n",
        "documents = ['Hello, how are you!',\n",
        "             'Win money, win from home.',\n",
        "             'Call me now.',\n",
        "             'Hello, Call hello you tomorrow?']\n",
        "\n",
        "lower_case_documents = []\n",
        "for i in documents:\n",
        "    lower_case_documents.append(i.lower())\n",
        "print(lower_case_documents)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello, how are you!', 'win money, win from home.', 'call me now.', 'hello, call hello you tomorrow?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx7IZa21uQF4"
      },
      "source": [
        "** Step 2: Removing all punctuations **\n",
        "\n",
        ">>**Instructions: **\n",
        "Remove all punctuation from the strings in the document set. Save them into a list called \n",
        "'sans_punctuation_documents'. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxIEM_vDuQF4",
        "outputId": "c7c137b8-151a-462a-f778-8887338c88c6"
      },
      "source": [
        "'''\n",
        "Solution:\n",
        "'''\n",
        "sans_punctuation_documents = []\n",
        "import string\n",
        "\n",
        "for i in lower_case_documents:\n",
        "    sans_punctuation_documents.append(i.translate(str.maketrans('', '', string.punctuation)))\n",
        "print(sans_punctuation_documents)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello how are you', 'win money win from home', 'call me now', 'hello call hello you tomorrow']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnGVae8QuQF5"
      },
      "source": [
        "** Step 3: Tokenization **\n",
        "\n",
        "Tokenizing a sentence in a document set means splitting up a sentence into individual words using a delimiter. The delimiter specifies what character we will use to identify the beginning and the end of a word(for example we could use a single space as the delimiter for identifying words in our document set.)\n",
        "\n",
        "Tokenizar uma frase em um conjunto de documentos significa dividir uma frase em palavras individuais usando um delimitador. O delimitador especifica qual caractere usaremos para identificar o início e o fim de uma palavra (por exemplo, podemos usar um único espaço como delimitador para identificar palavras em nosso conjunto de documentos)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "fF45_0xGuQF5"
      },
      "source": [
        ">>**Instructions:**\n",
        "Tokenize the strings stored in 'sans_punctuation_documents' using the split() method. and store the final document set \n",
        "in a list called 'preprocessed_documents'.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qv5pvnauQF5",
        "outputId": "4d8979b1-deff-4513-e542-e590f966b99d"
      },
      "source": [
        "'''\n",
        "Solution:\n",
        "'''\n",
        "preprocessed_documents = []\n",
        "for i in sans_punctuation_documents:\n",
        "    preprocessed_documents.append(i.split(' '))\n",
        "print(preprocessed_documents)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['hello', 'how', 'are', 'you'], ['win', 'money', 'win', 'from', 'home'], ['call', 'me', 'now'], ['hello', 'call', 'hello', 'you', 'tomorrow']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ot76uVUxuQF5"
      },
      "source": [
        "** Step 4: Count frequencies **\n",
        "\n",
        "Now that we have our document set in the required format, we can proceed to counting the occurrence of each word in each document of the document set. We will use the `Counter` method from the Python `collections` library for this purpose. \n",
        "\n",
        "Agora que temos nosso documento configurado no formato requerido, podemos prosseguir com a contagem da ocorrência de cada palavra em cada documento do conjunto de documentos. Usaremos o método `Counter` da biblioteca de` coleções` do Python para este propósito.\n",
        "\n",
        "`Counter` counts the occurrence of each item in the list and returns a dictionary with the key as the item being counted and the corresponding value being the count of that item in the list. \n",
        "\n",
        "`Contador` conta a ocorrência de cada item na lista e retorna um dicionário com a chave como o item sendo contado e o valor correspondente sendo a contagem desse item na lista."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "hFX6l10cuQF6"
      },
      "source": [
        ">>**Instructions:**\n",
        "Using the Counter() method and preprocessed_documents as the input, create a dictionary with the keys being each word in each document and the corresponding values being the frequncy of occurrence of that word. Save each Counter dictionary as an item in a list called 'frequency_list'.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pU3t0rRauQF6",
        "outputId": "3041a220-b26b-4e8f-a756-e3c730a3b4b1"
      },
      "source": [
        "'''\n",
        "Solution\n",
        "'''\n",
        "frequency_list = []\n",
        "import pprint\n",
        "from collections import Counter\n",
        "\n",
        "for i in preprocessed_documents:\n",
        "    frequency_counts = Counter(i)\n",
        "    frequency_list.append(frequency_counts)\n",
        "    \n",
        "pprint.pprint(frequency_list)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Counter({'hello': 1, 'how': 1, 'are': 1, 'you': 1}),\n",
            " Counter({'win': 2, 'money': 1, 'from': 1, 'home': 1}),\n",
            " Counter({'call': 1, 'me': 1, 'now': 1}),\n",
            " Counter({'hello': 2, 'call': 1, 'you': 1, 'tomorrow': 1})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDWMV2GduQF6"
      },
      "source": [
        "Congratulations! You have implemented the Bag of Words process from scratch! As we can see in our previous output, we have a frequency distribution dictionary which gives a clear view of the text that we are dealing with.\n",
        "\n",
        "Parabéns! Você implementou o processo Saco de Palavras do zero! Como podemos ver em nossa saída anterior, temos um dicionário de distribuição de frequência que dá uma visão clara do texto com o qual estamos lidando.\n",
        "\n",
        "We should now have a solid understanding of what is happening behind the scenes in the `sklearn.feature_extraction.text.CountVectorizer` method of scikit-learn. \n",
        "\n",
        "Agora devemos ter uma compreensão sólida do que está acontecendo nos bastidores no método `sklearn.feature_extraction.text.CountVectorizer` de scikit-learn.\n",
        "\n",
        "We will now implement `sklearn.feature_extraction.text.CountVectorizer` method in the next step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYa039e9uQF6"
      },
      "source": [
        "### Step 2.3: Implementing Bag of Words in scikit-learn ###\n",
        "\n",
        "Now that we have implemented the BoW concept from scratch, let's go ahead and use scikit-learn to do this process in a clean and succinct way. We will use the same document set as we used in the previous step. \n",
        "\n",
        "Agora que implementamos o conceito BoW do zero, vamos usar o scikit-learn para fazer esse processo de maneira limpa e sucinta. Usaremos o mesmo conjunto de documentos usado na etapa anterior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "iV_KvDTruQF7"
      },
      "source": [
        "'''\n",
        "Here we will look to create a frequency matrix on a smaller document set to make sure we understand how the \n",
        "document-term matrix generation happens. We have created a sample document set 'documents'.\n",
        "'''\n",
        "documents = ['Hello, how are you!',\n",
        "                'Win money, win from home.',\n",
        "                'Call me now.',\n",
        "                'Hello, Call hello you tomorrow?']"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0_08MtYuQF7"
      },
      "source": [
        ">>**Instructions:**\n",
        "Import the sklearn.feature_extraction.text.CountVectorizer method and create an instance of it called 'count_vector'. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "jU-AgBjzuQF7"
      },
      "source": [
        "'''\n",
        "Solution\n",
        "'''\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vector = CountVectorizer()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5cF7fRVuQF7"
      },
      "source": [
        "** Data preprocessing with CountVectorizer() **\n",
        "\n",
        "In Step 2.2, we implemented a version of the CountVectorizer() method from scratch that entailed cleaning our data first. This cleaning involved converting all of our data to lower case and removing all punctuation marks. CountVectorizer() has certain parameters which take care of these steps for us. They are:\n",
        "\n",
        "Na Etapa 2.2, implementamos uma versão do método CountVectorizer () do zero que envolvia a limpeza de nossos dados primeiro. Essa limpeza envolveu a conversão de todos os nossos dados em letras minúsculas e a remoção de todas as marcas de pontuação. CountVectorizer () tem certos parâmetros que cuidam dessas etapas para nós. Eles são:\n",
        "\n",
        "* `lowercase = True`\n",
        "    \n",
        "    The `lowercase` parameter has a default value of `True` which converts all of our text to its lower case form.\n",
        "\n",
        "\n",
        "* `token_pattern = (?u)\\\\b\\\\w\\\\w+\\\\b`\n",
        "    \n",
        "    The `token_pattern` parameter has a default regular expression value of `(?u)\\\\b\\\\w\\\\w+\\\\b` which ignores all punctuation marks and treats them as delimiters, while accepting alphanumeric strings of length greater than or equal to 2, as individual tokens or words.\n",
        "\n",
        "\n",
        "* `stop_words`\n",
        "\n",
        "    The `stop_words` parameter, if set to `english` will remove all words from our document set that match a list of English stop words which is defined in scikit-learn. Considering the size of our dataset and the fact that we are dealing with SMS messages and not larger text sources like e-mail, we will not be setting this parameter value.\n",
        "\n",
        "    O parâmetro `stop_words`, se definido como` english` irá remover todas as palavras de nosso conjunto de documentos que correspondem a uma lista de palavras de parada em inglês que é definida no scikit-learn. Considerando o tamanho de nosso conjunto de dados e o fato de que estamos lidando com mensagens SMS e não fontes de texto maiores como e-mail, não definiremos este valor de parâmetro.\n",
        "\n",
        "You can take a look at all the parameter values of your `count_vector` object by simply printing out the object as follows:\n",
        "\n",
        "Você pode dar uma olhada em todos os valores de parâmetro do seu objeto `count_vector` simplesmente imprimindo o objeto da seguinte forma:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfzjQxDLuQF8",
        "outputId": "30d3e646-7b2d-4bb4-8f9b-340c34fa763c"
      },
      "source": [
        "'''\n",
        "Practice node:\n",
        "Print the 'count_vector' object which is an instance of 'CountVectorizer()'\n",
        "'''\n",
        "print(count_vector)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CountVectorizer()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "jndSeRXIuQF8"
      },
      "source": [
        ">>**Instructions:**\n",
        "Fit your document dataset to the CountVectorizer object you have created using fit(), and get the list of words \n",
        "which have been categorized as features using the get_feature_names() method.\n",
        "\n",
        "Ajuste o conjunto de dados do documento ao objeto CountVectorizer que você criou usando fit () e obtenha a lista de palavras\n",
        "que foram categorizados como recursos usando o método get_feature_names ()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MssyZQu6uQF8",
        "outputId": "abd8a42e-f62e-4f24-ab32-3bb7ae92a1d2"
      },
      "source": [
        "'''\n",
        "Solution:\n",
        "'''\n",
        "count_vector.fit(documents)\n",
        "count_vector.get_feature_names()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['are',\n",
              " 'call',\n",
              " 'from',\n",
              " 'hello',\n",
              " 'home',\n",
              " 'how',\n",
              " 'me',\n",
              " 'money',\n",
              " 'now',\n",
              " 'tomorrow',\n",
              " 'win',\n",
              " 'you']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hST9xiQFuQF8"
      },
      "source": [
        "The `get_feature_names()` method returns our feature names for this dataset, which is the set of words that make up our vocabulary for 'documents'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooEzOAyzuQF8"
      },
      "source": [
        ">>**\n",
        "Instructions:**\n",
        "Create a matrix with the rows being each of the 4 documents, and the columns being each word. \n",
        "The corresponding (row, column) value is the frequency of occurrence of that word(in the column) in a particular\n",
        "document(in the row). You can do this using the transform() method and passing in the document data set as the \n",
        "argument. The transform() method returns a matrix of numpy integers, you can convert this to an array using\n",
        "toarray(). Call the array 'doc_array'\n",
        "\n",
        "Crie uma matriz com as linhas sendo cada um dos 4 documentos e as colunas sendo cada palavra.\n",
        "O valor correspondente (linha, coluna) é a frequência de ocorrência dessa palavra (na coluna) em um determinado\n",
        "documento (na linha). Você pode fazer isso usando o método transform () e passando o conjunto de dados do documento como o\n",
        "argumento. O método transform () retorna uma matriz de números inteiros, você pode convertê-la em uma matriz usando\n",
        "toarray (). Chame a matriz 'doc_array'\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBXuTPrquQF8",
        "outputId": "0731da9e-2bdc-4535-a724-c864e8ffa206"
      },
      "source": [
        "'''\n",
        "Solution\n",
        "'''\n",
        "doc_array = count_vector.transform(documents).toarray()\n",
        "doc_array"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1],\n",
              "       [0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 2, 0],\n",
              "       [0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n",
              "       [0, 1, 0, 2, 0, 0, 0, 0, 0, 1, 0, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMRE4MHquQF9"
      },
      "source": [
        "Now we have a clean representation of the documents in terms of the frequency distribution of the words in them. To make it easier to understand our next step is to convert this array into a dataframe and name the columns appropriately.\n",
        "\n",
        "Agora temos uma representação clara dos documentos em termos da distribuição de frequência das palavras neles. Para facilitar o entendimento, nossa próxima etapa é converter esse array em um dataframe e nomear as colunas de maneira apropriada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wa0w5j92uQF9"
      },
      "source": [
        ">>**Instructions:**\n",
        "Convert the array we obtained, loaded into 'doc_array', into a dataframe and set the column names to \n",
        "the word names(which you computed earlier using get_feature_names(). Call the dataframe 'frequency_matrix'.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "id": "pzWE94n9uQF9",
        "outputId": "e8357247-dbe2-4c81-f0ff-2c04e08e5cd0"
      },
      "source": [
        "'''\n",
        "Solution\n",
        "'''\n",
        "frequency_matrix = pd.DataFrame(doc_array, \n",
        "                                columns = count_vector.get_feature_names())\n",
        "frequency_matrix"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>are</th>\n",
              "      <th>call</th>\n",
              "      <th>from</th>\n",
              "      <th>hello</th>\n",
              "      <th>home</th>\n",
              "      <th>how</th>\n",
              "      <th>me</th>\n",
              "      <th>money</th>\n",
              "      <th>now</th>\n",
              "      <th>tomorrow</th>\n",
              "      <th>win</th>\n",
              "      <th>you</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   are  call  from  hello  home  how  me  money  now  tomorrow  win  you\n",
              "0    1     0     0      1     0    1   0      0    0         0    0    1\n",
              "1    0     0     1      0     1    0   0      1    0         0    2    0\n",
              "2    0     1     0      0     0    0   1      0    1         0    0    0\n",
              "3    0     1     0      2     0    0   0      0    0         1    0    1"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTGjqaTeuQF9"
      },
      "source": [
        "Congratulations! You have successfully implemented a Bag of Words problem for a document dataset that we created. \n",
        "\n",
        "Parabéns! Você implementou com êxito um problema Bag of Words para um conjunto de dados de documento que criamos.\n",
        "\n",
        "One potential issue that can arise from using this method out of the box is the fact that if our dataset of text is extremely large(say if we have a large collection of news articles or email data), there will be certain values that are more common that others simply due to the structure of the language itself. So for example words like 'is', 'the', 'an', pronouns, grammatical constructs etc could skew our matrix and affect our analyis. \n",
        "\n",
        "Um problema potencial que pode surgir do uso desse método pronto para uso é o fato de que, se nosso conjunto de dados de texto for extremamente grande (digamos, se tivermos uma grande coleção de artigos de notícias ou dados de e-mail), haverá certos valores que são mais comum que outros simplesmente devido à própria estrutura da língua. Assim, por exemplo, palavras como 'é', 'o', 'uma', pronomes, construções gramaticais, etc. podem distorcer nossa matriz e afetar nossa análise.\n",
        "\n",
        "There are a couple of ways to mitigate this. One way is to use the `stop_words` parameter and set its value to `english`. This will automatically ignore all words(from our input text) that are found in a built in list of English stop words in scikit-learn.\n",
        "\n",
        "Existem algumas maneiras de atenuar isso. Uma maneira é usar o parâmetro `stop_words` e definir seu valor para` inglês`. Isso irá ignorar automaticamente todas as palavras (do nosso texto de entrada) que são encontradas em uma lista integrada de palavras de parada em inglês no scikit-learn.\n",
        "\n",
        "Another way of mitigating this is by using the [tfidf](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer) method. This method is out of scope for the context of this lesson."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgK5rIjAuQF9"
      },
      "source": [
        "### Step 3.1: Training and testing sets ###\n",
        "\n",
        "Now that we have understood how to deal with the Bag of Words problem we can get back to our dataset and proceed with our analysis. Our first step in this regard would be to split our dataset into a training and testing set so we can test our model later. \n",
        "\n",
        "Agora que entendemos como lidar com o problema do Saco de Palavras, podemos voltar ao nosso conjunto de dados e prosseguir com nossa análise. Nossa primeira etapa a esse respeito seria dividir nosso conjunto de dados em um conjunto de treinamento e teste para que possamos testar nosso modelo mais tarde."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXc8faVVuQF-"
      },
      "source": [
        "\n",
        ">>**Instructions:**\n",
        "Split the dataset into a training and testing set by using the train_test_split method in sklearn. Split the data\n",
        "using the following variables:\n",
        "* `X_train` is our training data for the 'sms_message' column.\n",
        "* `y_train` is our training data for the 'label' column\n",
        "* `X_test` is our testing data for the 'sms_message' column.\n",
        "* `y_test` is our testing data for the 'label' column\n",
        "Print out the number of rows we have in each our training and testing data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VEeBLgtuQF-",
        "outputId": "0b1d4af7-13b6-4b29-e045-2b2d07d08859"
      },
      "source": [
        "'''\n",
        "Solution\n",
        "\n",
        "NOTE: sklearn.cross_validation will be deprecated soon to sklearn.model_selection \n",
        "'''\n",
        "# split into training and testing sets\n",
        "# USE from sklearn.model_selection import train_test_split to avoid seeing deprecation warning.\n",
        "# from sklearn.cross_validation import train_test_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['sms_message'], \n",
        "                                                    df['label'], \n",
        "                                                    random_state=1)\n",
        "\n",
        "print('Number of rows in the total set: {}'.format(df.shape[0]))\n",
        "print('Number of rows in the training set: {}'.format(X_train.shape[0]))\n",
        "print('Number of rows in the test set: {}'.format(X_test.shape[0]))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in the total set: 5572\n",
            "Number of rows in the training set: 4179\n",
            "Number of rows in the test set: 1393\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GW9_HmJbuQF-"
      },
      "source": [
        "### Step 3.2: Applying Bag of Words processing to our dataset. ###\n",
        "\n",
        "Now that we have split the data, our next objective is to follow the steps from Step 2: Bag of words and convert our data into the desired matrix format. To do this we will be using CountVectorizer() as we did before. There are two  steps to consider here:\n",
        "\n",
        "* Firstly, we have to fit our training data (`X_train`) into `CountVectorizer()` and return the matrix.\n",
        "* Secondly, we have to transform our testing data (`X_test`) to return the matrix. \n",
        "\n",
        "Note that `X_train` is our training data for the 'sms_message' column in our dataset and we will be using this to train our model. \n",
        "\n",
        "`X_test` is our testing data for the 'sms_message' column and this is the data we will be using(after transformation to a matrix) to make predictions on. We will then compare those predictions with `y_test` in a later step. \n",
        "\n",
        "For now, we have provided the code that does the matrix transformations for you!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "ZMG86sCYuQF-",
        "outputId": "ca8545f0-5c1f-46f2-a0b7-fd89e6b783f3"
      },
      "source": [
        "'''\n",
        "[Practice Node]\n",
        "\n",
        "The code for this segment is in 2 parts. Firstly, we are learning a vocabulary dictionary for the training data \n",
        "and then transforming the data into a document-term matrix; secondly, for the testing data we are only \n",
        "transforming the data into a document-term matrix.\n",
        "\n",
        "This is similar to the process we followed in Step 2.3\n",
        "\n",
        "We will provide the transformed data to students in the variables 'training_data' and 'testing_data'.\n",
        "'''"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n[Practice Node]\\n\\nThe code for this segment is in 2 parts. Firstly, we are learning a vocabulary dictionary for the training data \\nand then transforming the data into a document-term matrix; secondly, for the testing data we are only \\ntransforming the data into a document-term matrix.\\n\\nThis is similar to the process we followed in Step 2.3\\n\\nWe will provide the transformed data to students in the variables 'training_data' and 'testing_data'.\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Sj-YjLwyuQF-"
      },
      "source": [
        "'''\n",
        "Solution\n",
        "'''\n",
        "# Instantiate the CountVectorizer method\n",
        "count_vector = CountVectorizer()\n",
        "\n",
        "# Fit the training data and then return the matrix\n",
        "training_data = count_vector.fit_transform(X_train)\n",
        "\n",
        "# Transform testing data and return the matrix. Note we are not fitting the testing data into the CountVectorizer()\n",
        "testing_data = count_vector.transform(X_test)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yc5M_-1SuQF-"
      },
      "source": [
        "### Step 4.1: Bayes Theorem implementation from scratch ###\n",
        "\n",
        "Now that we have our dataset in the format that we need, we can move onto the next portion of our mission which is the  algorithm we will use to make our predictions to classify a message as spam or not spam. Remember that at the start of the mission we briefly discussed the Bayes theorem but now we shall go into a little more detail. In layman's terms, the Bayes theorem calculates the probability of an event occurring, based on certain other probabilities that are related to the event in question. It is  composed of a  prior(the probabilities that we are aware of or that is given to us) and the posterior(the probabilities we are looking to compute using the priors). \n",
        "\n",
        "Agora que temos nosso conjunto de dados no formato que precisamos, podemos passar para a próxima parte de nossa missão, que é o algoritmo que usaremos para fazer nossas previsões para classificar uma mensagem como spam ou não spam. Lembre-se de que, no início da missão, discutimos brevemente o teorema de Bayes, mas agora entraremos em mais detalhes. Em termos leigos, o teorema de Bayes calcula a probabilidade de um evento ocorrer, com base em certas outras probabilidades relacionadas ao evento em questão. É composto por um prior (as probabilidades de que temos conhecimento ou que nos são dados) e um posterior (as probabilidades que pretendemos calcular usando os antecedentes).\n",
        "\n",
        "Let us implement the Bayes Theorem from scratch using a simple example. Let's say we are trying to find the odds of an individual having diabetes, given that he or she was tested for it and got a positive result. \n",
        "\n",
        "Vamos implementar o Teorema de Bayes do zero usando um exemplo simples. Digamos que estejamos tentando encontrar as chances de um indivíduo ter diabetes, visto que ele foi testado e obteve um resultado positivo.\n",
        "\n",
        "In the medical field, such probabilies play a very important role as it usually deals with life and death situations. \n",
        "\n",
        "No campo médico, tais probabilidades desempenham um papel muito importante, pois geralmente tratam de situações de vida ou morte.\n",
        "\n",
        "We assume the following:\n",
        "\n",
        "`P(D)` is the probability of a person having Diabetes. It's value is `0.01` or in other words, 1% of the general population has diabetes(Disclaimer: these values are assumptions and are not reflective of any medical study).\n",
        "\n",
        "`P(Pos)` is the probability of getting a positive test result.\n",
        "\n",
        "`P(Neg)` is the probability of getting a negative test result.\n",
        "\n",
        "`P(Pos|D)` is the probability of getting a positive result on a test done for detecting diabetes, given that you have diabetes. This has a value `0.9`. In other words the test is correct 90% of the time. This is also called the Sensitivity or True Positive Rate.\n",
        "Isso também é chamado de Sensibilidade ou Taxa de Positivo Verdadeiro.\n",
        "\n",
        "`P(Neg|~D)` is the probability of getting a negative result on a test done for detecting diabetes, given that you do not have diabetes. This also has a value of `0.9` and is therefore correct, 90% of the time. This is also called the Specificity or True Negative Rate.\n",
        "Isso também é chamado de Especificidade ou Taxa Negativa Verdadeira.\n",
        "\n",
        "The Bayes formula is as follows:\n",
        "\n",
        "<img src=\"images/bayes_formula.png\" height=\"242\" width=\"242\">\n",
        "\n",
        "* `P(A)` is the prior probability of A occurring independently. In our example this is `P(D)`. This value is given to us.\n",
        "\n",
        "* `P(B)` is the prior probability of B occurring independently. In our example this is `P(Pos)`.\n",
        "\n",
        "* `P(A|B)` is the posterior probability that A occurs given B. In our example this is `P(D|Pos)`. That is, **the probability of an individual having diabetes, given that, that individual got a positive test result. This is the value that we are looking to calculate.**\n",
        "\n",
        "* `P(B|A)` is the likelihood probability of B occurring, given A. In our example this is `P(Pos|D)`. This value is given to us."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSGE3wU2uQF_"
      },
      "source": [
        "Putting our values into the formula for Bayes theorem we get:\n",
        "\n",
        "`P(D|Pos) = (P(D) * P(Pos|D) / P(Pos)`\n",
        "\n",
        "The probability of getting a positive test result `P(Pos)` can be calculated using the Sensitivity and Specificity as follows:\n",
        "\n",
        "`P(Pos) = [P(D) * Sensitivity] + [P(~D) * (1-Specificity))]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "p-BSjJ5AuQF_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d548ac5c-6357-4e93-c590-5bacc1094a52"
      },
      "source": [
        "'''\n",
        "Instructions:\n",
        "Calculate probability of getting a positive test result, P(Pos)\n",
        "'''"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nInstructions:\\nCalculate probability of getting a positive test result, P(Pos)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eunQ1cPuQF_",
        "outputId": "58e345b7-f32c-4934-f766-0834e922dd9e"
      },
      "source": [
        "'''\n",
        "Solution (skeleton code will be provided)\n",
        "'''\n",
        "# P(D)\n",
        "p_diabetes = 0.01\n",
        "\n",
        "# P(~D)\n",
        "p_no_diabetes = 0.99\n",
        "\n",
        "# Sensitivity or P(Pos|D)\n",
        "p_pos_diabetes = 0.9\n",
        "\n",
        "# Specificity or P(Neg/~D)\n",
        "p_neg_no_diabetes = 0.9\n",
        "\n",
        "# P(Pos)\n",
        "p_pos = (p_diabetes * p_pos_diabetes) + (p_no_diabetes * (1 - p_neg_no_diabetes))\n",
        "print('The probability of getting a positive test result P(Pos) is: {}',format(p_pos))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The probability of getting a positive test result P(Pos) is: {} 0.10799999999999998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jQzDkz8uQF_"
      },
      "source": [
        "** Using all of this information we can calculate our posteriors as follows: **\n",
        "    \n",
        "The probability of an individual having diabetes, given that, that individual got a positive test result:\n",
        "\n",
        "`P(D/Pos) = (P(D) * Sensitivity)) / P(Pos)`\n",
        "\n",
        "The probability of an individual not having diabetes, given that, that individual got a positive test result:\n",
        "\n",
        "`P(~D/Pos) = (P(~D) * (1-Specificity)) / P(Pos)`\n",
        "\n",
        "The sum of our posteriors will always equal `1`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "S3quiSaMuQF_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "142e74a9-e313-4dff-c349-870991d93e4b"
      },
      "source": [
        "'''\n",
        "Instructions:\n",
        "Compute the probability of an individual having diabetes, given that, that individual got a positive test result.\n",
        "In other words, compute P(D|Pos).\n",
        "\n",
        "The formula is: P(D|Pos) = (P(D) * P(Pos|D) / P(Pos)\n",
        "'''"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nInstructions:\\nCompute the probability of an individual having diabetes, given that, that individual got a positive test result.\\nIn other words, compute P(D|Pos).\\n\\nThe formula is: P(D|Pos) = (P(D) * P(Pos|D) / P(Pos)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gq2CscVBuQF_",
        "outputId": "9f4a6c67-7e92-45c2-eda4-aca60b2471c1"
      },
      "source": [
        "'''\n",
        "Solution\n",
        "'''\n",
        "# P(D|Pos)\n",
        "p_diabetes_pos = (p_diabetes * p_pos_diabetes) / p_pos\n",
        "print('Probability of an individual having diabetes, given that that individual got a positive test result is:\\\n",
        "',format(p_diabetes_pos)) "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probability of an individual having diabetes, given that that individual got a positive test result is: 0.08333333333333336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "hGNWouWIuQGA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "26ef8906-23e2-47c4-f952-04756ccf11f3"
      },
      "source": [
        "'''\n",
        "Instructions:\n",
        "Compute the probability of an individual not having diabetes, given that, that individual got a positive test result.\n",
        "In other words, compute P(~D|Pos).\n",
        "\n",
        "The formula is: P(~D|Pos) = (P(~D) * P(Pos|~D) / P(Pos)\n",
        "\n",
        "Note that P(Pos/~D) can be computed as 1 - P(Neg/~D). \n",
        "\n",
        "Therefore:\n",
        "P(Pos/~D) = p_pos_no_diabetes = 1 - 0.9 = 0.1\n",
        "'''"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nInstructions:\\nCompute the probability of an individual not having diabetes, given that, that individual got a positive test result.\\nIn other words, compute P(~D|Pos).\\n\\nThe formula is: P(~D|Pos) = (P(~D) * P(Pos|~D) / P(Pos)\\n\\nNote that P(Pos/~D) can be computed as 1 - P(Neg/~D). \\n\\nTherefore:\\nP(Pos/~D) = p_pos_no_diabetes = 1 - 0.9 = 0.1\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fNAY04NuQGA",
        "outputId": "afd9fab5-7bee-43e1-d0c5-0ffefa209754"
      },
      "source": [
        "'''\n",
        "Solution\n",
        "'''\n",
        "# P(Pos/~D)\n",
        "p_pos_no_diabetes = 0.1\n",
        "\n",
        "# P(~D|Pos)\n",
        "p_no_diabetes_pos = (p_no_diabetes * p_pos_no_diabetes) / p_pos\n",
        "print('Probability of an individual not having diabetes, given that that individual got a positive test result is: {}',format(p_no_diabetes_pos))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probability of an individual not having diabetes, given that that individual got a positive test result is: {} 0.9166666666666669\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBgzxlFsuQGA"
      },
      "source": [
        "Congratulations! You have implemented Bayes theorem from scratch. Your analysis shows that even if you get a positive test result, there is only a 8.3% chance that you actually have diabetes and a 91.67% chance that you do not have diabetes. This is of course assuming that only 1% of the entire population has diabetes which of course is only an assumption.\n",
        "\n",
        "Parabéns! Você implementou o teorema de Bayes do zero. Sua análise mostra que mesmo se você obtiver um resultado positivo no teste, há apenas 8,3% de chance de você realmente ter diabetes e 91,67% de chance de não ter diabetes. É claro que isso pressupõe que apenas 1% de toda a população tem diabetes, o que, obviamente, é apenas uma suposição."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvRZbdVYuQGA"
      },
      "source": [
        "** What does the term 'Naive' in 'Naive Bayes' mean ? ** \n",
        "\n",
        "The term 'Naive' in Naive Bayes comes from the fact that the algorithm considers the features that it is using to make the predictions to be independent of each other, which may not always be the case. So in our Diabetes example, we are considering only one feature, that is the test result. Say we added another feature, 'exercise'. Let's say this feature has a binary value of `0` and `1`, where the former signifies that the individual exercises less than or equal to 2 days a week and the latter signifies that the individual exercises greater than or equal to 3 days a week. If we had to use both of these features, namely the test result and the value of the 'exercise' feature, to compute our final probabilities, Bayes' theorem would fail. Naive Bayes' is an extension of Bayes' theorem that assumes that all the features are independent of each other. \n",
        "\n",
        "O termo 'Naive' em Naive Bayes vem do fato de que o algoritmo considera os recursos que está usando para fazer as previsões como independentes uns dos outros, o que pode nem sempre ser o caso. Portanto, em nosso exemplo do Diabetes, estamos considerando apenas um recurso, que é o resultado do teste. Digamos que adicionamos outro recurso, 'exercício'. Digamos que este recurso tenha um valor binário de `0` e` 1`, onde o primeiro significa que o indivíduo se exercita menor ou igual a 2 dias por semana e o último significa que o indivíduo se exercita maior ou igual a 3 dias a semana. Se tivéssemos que usar esses dois recursos, ou seja, o resultado do teste e o valor do recurso 'exercício', para calcular nossas probabilidades finais, o teorema de Bayes falharia. Naive Bayes 'é uma extensão do teorema de Bayes que assume que todas as características são independentes umas das outras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOZF_H1huQGA"
      },
      "source": [
        "### Step 4.2: Naive Bayes implementation from scratch ###\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDGK0s8ZuQGA"
      },
      "source": [
        "Now that you have understood the ins and outs of Bayes Theorem, we will extend it to consider cases where we have more than one feature. \n",
        "\n",
        "Agora que você entendeu os meandros do Teorema de Bayes, vamos estendê-lo para considerar os casos em que temos mais de um recurso.\n",
        "\n",
        "Let's say that we have two political parties' candidates, 'Jill Stein' of the Green Party and 'Gary Johnson' of the Libertarian Party and we have the probabilities of each of these candidates saying the words 'freedom', 'immigration' and 'environment' when they give a speech:\n",
        "\n",
        "Digamos que temos dois candidatos de partidos políticos, 'Jill Stein' do Partido Verde e 'Gary Johnson' do Partido Libertário e temos as probabilidades de cada um desses candidatos dizer as palavras 'liberdade', 'imigração' e ' ambiente 'quando fazem um discurso:\n",
        "\n",
        "* Probability that Jill Stein says 'freedom': 0.1 ---------> `P(F|J)`\n",
        "* Probability that Jill Stein says 'immigration': 0.1 -----> `P(I|J)`\n",
        "* Probability that Jill Stein says 'environment': 0.8 -----> `P(E|J)`\n",
        "\n",
        "\n",
        "* Probability that Gary Johnson says 'freedom': 0.7 -------> `P(F|G)`\n",
        "* Probability that Gary Johnson says 'immigration': 0.2 ---> `P(I|G)`\n",
        "* Probability that Gary Johnson says 'environment': 0.1 ---> `P(E|G)`\n",
        "\n",
        "\n",
        "And let us also assume that the probability of Jill Stein giving a speech, `P(J)` is `0.5` and the same for Gary Johnson, `P(G) = 0.5`. \n",
        "\n",
        "\n",
        "Given this, what if we had to find the probabilities of Jill Stein saying the words 'freedom' and 'immigration'? This is where the Naive Bayes'theorem comes into play as we are considering two features, 'freedom' and 'immigration'.\n",
        "\n",
        "Now we are at a place where we can define the formula for the Naive Bayes' theorem:\n",
        "\n",
        "<img src=\"images/naivebayes.png\" height=\"342\" width=\"342\">\n",
        "\n",
        "Here, `y` is the class variable or in our case the name of the candidate and `x1` through `xn` are the feature vectors or in our case the individual words. The theorem makes the assumption that each of the feature vectors or words (`xi`) are independent of each other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJxWXh6RuQGA"
      },
      "source": [
        "To break this down, we have to compute the following posterior probabilities:\n",
        "\n",
        "* `P(J|F,I)`: Probability of Jill Stein saying the words Freedom and Immigration. \n",
        "\n",
        "    Using the formula and our knowledge of Bayes' theorem, we can compute this as follows: `P(J|F,I)` = `(P(J) * P(F|J) * P(I|J)) / P(F,I)`. Here `P(F,I)` is the probability of the words 'freedom' and 'immigration' being said in a speech.\n",
        "    \n",
        "\n",
        "* `P(G|F,I)`: Probability of Gary Johnson saying the words Freedom and Immigration.  \n",
        "    \n",
        "    Using the formula, we can compute this as follows: `P(G|F,I)` = `(P(G) * P(F|G) * P(I|G)) / P(F,I)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "XKTDYuCQuQGB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "6985ac8e-8fa6-4435-f6d6-6fd5c702d921"
      },
      "source": [
        "'''\n",
        "Instructions: Compute the probability of the words 'freedom' and 'immigration' being said in a speech, or\n",
        "P(F,I).\n",
        "\n",
        "The first step is multiplying the probabilities of Jill Stein giving a speech with her individual \n",
        "probabilities of saying the words 'freedom' and 'immigration'. Store this in a variable called p_j_text\n",
        "\n",
        "The second step is multiplying the probabilities of Gary Johnson giving a speech with his individual \n",
        "probabilities of saying the words 'freedom' and 'immigration'. Store this in a variable called p_g_text\n",
        "\n",
        "The third step is to add both of these probabilities and you will get P(F,I).\n",
        "'''"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nInstructions: Compute the probability of the words 'freedom' and 'immigration' being said in a speech, or\\nP(F,I).\\n\\nThe first step is multiplying the probabilities of Jill Stein giving a speech with her individual \\nprobabilities of saying the words 'freedom' and 'immigration'. Store this in a variable called p_j_text\\n\\nThe second step is multiplying the probabilities of Gary Johnson giving a speech with his individual \\nprobabilities of saying the words 'freedom' and 'immigration'. Store this in a variable called p_g_text\\n\\nThe third step is to add both of these probabilities and you will get P(F,I).\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ywU6QW8uQGB",
        "outputId": "dfa8869f-8e30-4ae2-b9e4-346df3a79d91"
      },
      "source": [
        "'''\n",
        "Solution: Step 1\n",
        "'''\n",
        "# P(J)\n",
        "p_j = 0.5\n",
        "\n",
        "# P(F/J)\n",
        "p_j_f = 0.1\n",
        "\n",
        "# P(I/J)\n",
        "p_j_i = 0.1\n",
        "\n",
        "p_j_text = p_j * p_j_f * p_j_i\n",
        "print(p_j_text)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.005000000000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaQJD9uyuQGB",
        "outputId": "db398014-2993-4a61-c726-0ef60970412e"
      },
      "source": [
        "'''\n",
        "Solution: Step 2\n",
        "'''\n",
        "# P(G)\n",
        "p_g = 0.5\n",
        "\n",
        "# P(F/G)\n",
        "p_g_f = 0.7\n",
        "\n",
        "# P(I/G)\n",
        "p_g_i = 0.2\n",
        "\n",
        "p_g_text = p_g * p_g_f * p_g_i\n",
        "print(p_g_text)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.06999999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2FM6qBWuQGB",
        "outputId": "d793dde2-55ab-4333-b2ca-2221f5fde378"
      },
      "source": [
        "'''\n",
        "Solution: Step 3: Compute P(F,I) and store in p_f_i\n",
        "'''\n",
        "p_f_i = p_j_text + p_g_text\n",
        "print('Probability of words freedom and immigration being said are: ', format(p_f_i))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probability of words freedom and immigration being said are:  0.075\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnmsTro2uQGB"
      },
      "source": [
        "Now we can compute the probability of `P(J|F,I)`, that is the probability of Jill Stein saying the words Freedom and Immigration and `P(G|F,I)`, that is the probability of Gary Johnson saying the words Freedom and Immigration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "TxQSJI79uQGC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9cb51cfd-1f14-4f1f-98d9-d3356f092a9c"
      },
      "source": [
        "'''\n",
        "Instructions:\n",
        "Compute P(J|F,I) using the formula P(J|F,I) = (P(J) * P(F|J) * P(I|J)) / P(F,I) and store it in a variable p_j_fi\n",
        "'''"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nInstructions:\\nCompute P(J|F,I) using the formula P(J|F,I) = (P(J) * P(F|J) * P(I|J)) / P(F,I) and store it in a variable p_j_fi\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "An4RG8cLuQGC",
        "outputId": "8eb32d9d-f987-4a9e-b010-2932761e9bce"
      },
      "source": [
        "'''\n",
        "Solution\n",
        "'''\n",
        "p_j_fi = p_j_text / p_f_i\n",
        "print('The probability of Jill Stein saying the words Freedom and Immigration: ', format(p_j_fi))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The probability of Jill Stein saying the words Freedom and Immigration:  0.06666666666666668\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "9Rd9RCIluQGC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c733b824-dda1-47a0-aa9d-75137ea38793"
      },
      "source": [
        "'''\n",
        "Instructions:\n",
        "Compute P(G|F,I) using the formula P(G|F,I) = (P(G) * P(F|G) * P(I|G)) / P(F,I) and store it in a variable p_g_fi\n",
        "'''"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nInstructions:\\nCompute P(G|F,I) using the formula P(G|F,I) = (P(G) * P(F|G) * P(I|G)) / P(F,I) and store it in a variable p_g_fi\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YatL9hxuQGC",
        "outputId": "14ffe805-2760-4a1f-e8c5-89390128b085"
      },
      "source": [
        "'''\n",
        "Solution\n",
        "'''\n",
        "p_g_fi = p_g_text / p_f_i\n",
        "print('The probability of Gary Johnson saying the words Freedom and Immigration: ', format(p_g_fi))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The probability of Gary Johnson saying the words Freedom and Immigration:  0.9333333333333332\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3mIGXPruQGC"
      },
      "source": [
        "And as we can see, just like in the Bayes' theorem case, the sum of our posteriors is equal to 1. Congratulations! You have implemented the Naive Bayes' theorem from scratch. Our analysis shows that there is only a 6.6% chance that Jill Stein of the Green Party uses the words 'freedom' and 'immigration' in her speech as compared the the 93.3% chance for Gary Johnson of the Libertarian party.\n",
        "\n",
        "E como podemos ver, assim como no caso do teorema de Bayes, a soma de nossos posteriores é igual a 1. Parabéns! Você implementou o teorema de Naive Bayes do zero. Nossa análise mostra que há apenas 6,6% de chance de que Jill Stein, do Partido Verde, use as palavras 'liberdade' e 'imigração' em seu discurso, em comparação com a chance de 93,3% de Gary Johnson, do Partido Libertário."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQHjUIo-uQGC"
      },
      "source": [
        "Another more generic example of Naive Bayes' in action is as when we search for the term 'Sacramento Kings' in a search engine. In order for us to get the results pertaining to the Scramento Kings NBA basketball team, the search engine needs to be able to associate the two words together and not treat them individually, in which case we would get results of images tagged with 'Sacramento' like pictures of city landscapes and images of 'Kings' which could be pictures of crowns or kings from history when what we are looking to get are images of the basketball team. This is a classic case of the search engine treating the words as independent entities and hence being 'naive' in its approach. \n",
        "\n",
        "Outro exemplo mais genérico de Naive Bayes 'em ação é como quando pesquisamos o termo' Sacramento Kings 'em um mecanismo de busca. Para que possamos obter os resultados relativos ao time de basquete Scramento Kings da NBA, o mecanismo de pesquisa deve ser capaz de associar as duas palavras e não tratá-las individualmente. Nesse caso, obteríamos resultados de imagens marcadas com 'Sacramento' como imagens de paisagens de cidades e imagens de 'Reis' que poderiam ser imagens de coroas ou reis da história quando o que estamos procurando são imagens do time de basquete. Este é um caso clássico em que o mecanismo de pesquisa trata as palavras como entidades independentes e, portanto, é \"ingênuo\" em sua abordagem.\n",
        "\n",
        "Applying this to our problem of classifying messages as spam, the Naive Bayes algorithm *looks at each word individually and not as associated entities* with any kind of link between them. In the case of spam detectors, this usually works as there are certain red flag words which can almost guarantee its classification as spam, for example emails with words like 'viagra' are usually classified as spam.\n",
        "\n",
        "Aplicando isso ao nosso problema de classificação de mensagens como spam, o algoritmo Naive Bayes * olha para cada palavra individualmente e não como entidades associadas * com qualquer tipo de link entre elas. No caso de detectores de spam, isso geralmente funciona, pois há certas palavras de bandeira vermelha que quase garantem sua classificação como spam, por exemplo, e-mails com palavras como 'viagra' são geralmente classificados como spam."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycLng8WTuQGC"
      },
      "source": [
        "### Step 5: Naive Bayes implementation using scikit-learn ###\n",
        "\n",
        "Thankfully, sklearn has several Naive Bayes implementations that we can use and so we do not have to do the math from scratch. We will be using sklearns `sklearn.naive_bayes` method to make predictions on our dataset.\n",
        "\n",
        "Felizmente, sklearn tem várias implementações Naive Bayes que podemos usar e, portanto, não temos que fazer as contas do zero. Estaremos usando o método sklearns `sklearn.naive_bayes` para fazer previsões em nosso conjunto de dados.\n",
        "\n",
        "Specifically, we will be using the multinomial Naive Bayes implementation. This particular classifier is suitable for classification with discrete features (such as in our case, word counts for text classification). It takes in integer word counts as its input. On the other hand Gaussian Naive Bayes is better suited for continuous data as it assumes that the input data has a Gaussian(normal) distribution.\n",
        "\n",
        "Especificamente, estaremos usando a implementação multinomial Naive Bayes. Este classificador particular é adequado para classificação com recursos discretos (como em nosso caso, contagem de palavras para classificação de texto). Ele recebe contagens de palavras inteiras como sua entrada. Por outro lado, Gaussian Naive Bayes é mais adequado para dados contínuos, pois assume que os dados de entrada têm uma distribuição Gaussiana (normal)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "oQNW0jwjuQGC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "d8951434-98d6-42ef-dbb1-611cd60b4abc"
      },
      "source": [
        "'''\n",
        "Instructions:\n",
        "\n",
        "We have loaded the training data into the variable 'training_data' and the testing data into the \n",
        "variable 'testing_data'.\n",
        "\n",
        "Import the MultinomialNB classifier and fit the training data into the classifier using fit(). Name your classifier\n",
        "'naive_bayes'. You will be training the classifier using 'training_data' and y_train' from our split earlier. \n",
        "'''"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nInstructions:\\n\\nWe have loaded the training data into the variable 'training_data' and the testing data into the \\nvariable 'testing_data'.\\n\\nImport the MultinomialNB classifier and fit the training data into the classifier using fit(). Name your classifier\\n'naive_bayes'. You will be training the classifier using 'training_data' and y_train' from our split earlier. \\n\""
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWIt2KyvuQGD",
        "outputId": "350db471-cccc-421e-d4e8-e25e38f75ef3"
      },
      "source": [
        "'''\n",
        "Solution\n",
        "'''\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "naive_bayes = MultinomialNB()\n",
        "naive_bayes.fit(training_data, y_train)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "FqMw2KEguQGD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "29b2e349-dafb-4068-f5d0-388e9596b258"
      },
      "source": [
        "'''\n",
        "Instructions:\n",
        "Now that our algorithm has been trained using the training data set we can now make some predictions on the test data\n",
        "stored in 'testing_data' using predict(). Save your predictions into the 'predictions' variable.\n",
        "'''"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nInstructions:\\nNow that our algorithm has been trained using the training data set we can now make some predictions on the test data\\nstored in 'testing_data' using predict(). Save your predictions into the 'predictions' variable.\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "JOa-rgiUuQGD"
      },
      "source": [
        "'''\n",
        "Solution\n",
        "'''\n",
        "predictions = naive_bayes.predict(testing_data)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLv6MZ7ouQGD"
      },
      "source": [
        "Now that predictions have been made on our test set, we need to check the accuracy of our predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNWYDBfXuQGD"
      },
      "source": [
        "### Step 6: Evaluating our model ###\n",
        "\n",
        "Now that we have made predictions on our test set, our next goal is to evaluate how well our model is doing. There are various mechanisms for doing so, but first let's do quick recap of them.\n",
        "\n",
        "Agora que fizemos previsões em nosso conjunto de teste, nosso próximo objetivo é avaliar o quão bem nosso modelo está indo. Existem vários mecanismos para fazer isso, mas primeiro vamos fazer uma recapitulação rápida deles.\n",
        "\n",
        "** Accuracy ** measures how often the classifier makes the correct prediction. It’s the ratio of the number of correct predictions to the total number of predictions (the number of test data points).\n",
        "\n",
        "** Accuracy ** mede a frequência com que o classificador faz a previsão correta. É a razão entre o número de previsões corretas e o número total de previsões (o número de pontos de dados de teste).\n",
        "\n",
        "** Precision ** tells us what proportion of messages we classified as spam, actually were spam.\n",
        "It is a ratio of true positives(words classified as spam, and which are actually spam) to all positives(all words classified as spam, irrespective of whether that was the correct classification), in other words it is the ratio of\n",
        "\n",
        "** Precision ** nos informa a proporção de mensagens que classificamos como spam, na verdade eram spam.\n",
        "É uma proporção de verdadeiros positivos (palavras classificadas como spam, e que são realmente spam) para todos os positivos (todas as palavras classificadas como spam, independentemente de ser essa a classificação correta), em outras palavras, é a proporção de\n",
        "\n",
        "\n",
        "`[True Positives/(True Positives + False Positives)]`\n",
        "\n",
        "** Recall(sensitivity)** tells us what proportion of messages that actually were spam were classified by us as spam.\n",
        "It is a ratio of true positives(words classified as spam, and which are actually spam) to all the words that were actually spam, in other words it is the ratio of\n",
        "\n",
        "** Recall (sensibilidade) ** nos informa a proporção de mensagens que realmente eram spam foram classificadas por nós como spam.\n",
        "É uma proporção de verdadeiros positivos (palavras classificadas como spam, e que são realmente spam) para todas as palavras que foram realmente spam, em outras palavras, é a proporção de\n",
        "\n",
        "`[True Positives/(True Positives + False Negatives)]`\n",
        "\n",
        "For classification problems that are skewed in their classification distributions like in our case, for example if we had a 100 text messages and only 2 were spam and the rest 98 weren't, accuracy by itself is not a very good metric. We could classify 90 messages as not spam(including the 2 that were spam but we classify them as not spam, hence they would be false negatives) and 10 as spam(all 10 false positives) and still get a reasonably good accuracy score. For such cases, precision and recall come in very handy. These two metrics can be combined to get the F1 score, which is weighted average of the precision and recall scores. This score can range from 0 to 1, with 1 being the best possible F1 score.\n",
        "\n",
        "Para problemas de classificação que são distorcidos em suas distribuições de classificação, como em nosso caso, por exemplo, se tivéssemos 100 mensagens de texto e apenas 2 fossem spam e as 98 restantes não, a precisão por si só não é uma métrica muito boa. Poderíamos classificar 90 mensagens como não spam (incluindo as 2 que eram spam, mas nós as classificamos como não spam, portanto, seriam falsos negativos) e 10 como spam (todos os 10 falsos positivos) e ainda obter uma pontuação de precisão razoavelmente boa. Para tais casos, precisão e recall são muito úteis. Essas duas métricas podem ser combinadas para obter a pontuação F1, que é a média ponderada das pontuações de precisão e recall. Essa pontuação pode variar de 0 a 1, sendo 1 a melhor pontuação possível na F1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxP9qz85uQGD"
      },
      "source": [
        "We will be using all 4 metrics to make sure our model does well. For all 4 metrics whose values can range from 0 to 1, having a score as close to 1 as possible is a good indicator of how well our model is doing.\n",
        "\n",
        "Usaremos todas as 4 métricas para garantir que nosso modelo funcione bem. Para todas as 4 métricas cujos valores podem variar de 0 a 1, ter uma pontuação o mais próxima possível de 1 é um bom indicador de como nosso modelo está indo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "QKBDMbBFuQGE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "bd28b51d-f181-4f32-bbfc-6a08d5646bc1"
      },
      "source": [
        "'''\n",
        "Instructions:\n",
        "Compute the accuracy, precision, recall and F1 scores of your model using your test data 'y_test' and the predictions\n",
        "you made earlier stored in the 'predictions' variable.\n",
        "'''"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nInstructions:\\nCompute the accuracy, precision, recall and F1 scores of your model using your test data 'y_test' and the predictions\\nyou made earlier stored in the 'predictions' variable.\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhhC23FWuQGE",
        "outputId": "e24c3afa-9a11-4728-fdd6-9b3a92cb3058"
      },
      "source": [
        "'''\n",
        "Solution\n",
        "'''\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "print('Accuracy score: ', format(accuracy_score(y_test, predictions)))\n",
        "print('Precision score: ', format(precision_score(y_test, predictions)))\n",
        "print('Recall score: ', format(recall_score(y_test, predictions)))\n",
        "print('F1 score: ', format(f1_score(y_test, predictions)))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy score:  0.9885139985642498\n",
            "Precision score:  0.9720670391061452\n",
            "Recall score:  0.9405405405405406\n",
            "F1 score:  0.9560439560439562\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "eUgqD1vIuQGE"
      },
      "source": [
        "### Step 7: Conclusion ###\n",
        "\n",
        "One of the major advantages that Naive Bayes has over other classification algorithms is its ability to handle an extremely large number of features. In our case, each word is treated as a feature and there are thousands of different words. Also, it performs well even with the presence of irrelevant features and is relatively unaffected by them. The other major advantage it has is its relative simplicity. Naive Bayes' works well right out of the box and tuning it's parameters is rarely ever necessary, except usually in cases where the distribution of the data is known. \n",
        "It rarely ever overfits the data. Another important advantage is that its model training and prediction times are very fast for the amount of data it can handle. All in all, Naive Bayes' really is a gem of an algorithm!\n",
        "\n",
        "Uma das principais vantagens do Naive Bayes sobre outros algoritmos de classificação é sua capacidade de lidar com um número extremamente grande de recursos. No nosso caso, cada palavra é tratada como um recurso e existem milhares de palavras diferentes. Além disso, ele tem um bom desempenho mesmo com a presença de recursos irrelevantes e é relativamente afetado por eles. A outra grande vantagem que possui é sua relativa simplicidade. O Naive Bayes funciona bem fora da caixa e o ajuste de seus parâmetros raramente é necessário, exceto geralmente nos casos em que a distribuição dos dados é conhecida.\n",
        "Raramente supera os dados. Outra vantagem importante é que os tempos de treinamento e previsão do modelo são muito rápidos para a quantidade de dados que ele pode manipular. Ao todo, Naive Bayes 'é realmente uma joia de um algoritmo!\n",
        "\n",
        "Congratulations! You have successfully designed a model that can efficiently predict if an SMS message is spam or not!\n",
        "\n",
        "Parabéns! Você projetou com sucesso um modelo que pode prever com eficiência se uma mensagem SMS é spam ou não!\n",
        "\n",
        "Thank you for learning with us!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questionamentos:\n",
        "\n",
        "#1- O quê significa o \"Bag of Words\"?\n",
        "\n",
        "O conceito de Bag of Words (BoW) é um termo usado quando uma coleção de dados de texto que precisam ser trabalhados. A ideia básica do BoW é pegar um pedaço de texto e contar a frequência das palavras nesse texto. É importante notar que o conceito BoW trata cada palavra individualmente e a ordem em que as palavras ocorrem não importa.\n",
        "\n",
        "#2- Qual é a diferença entre especificidade e sensitividade?\n",
        "\n",
        "A Sensibilidade e a Especificidade descrevem a precisão de um teste que relata a presença ou ausência de uma condição, em comparação com um 'padrão' ou definição.\n",
        "\n",
        "A Sensibilidade ou Taxa de Verdadeiro Positivo é a probabilidade de obter resultado positivo em um teste uma vez que o mesmo é positivo.\n",
        "\n",
        "A Especificidade ou Taxa de Verdadeiro Negativo é probabilidade de obter resultado negativo em um teste uma vez que o mesmo é negativo.\n",
        "\n",
        "Em um teste de diagnóstico, a sensibilidade é uma medida de um teste poder identificar verdadeiros positivos e a especificidade é uma medida de um teste poder identificar verdadeiros negativos.\n",
        "\n",
        "Se o objetivo do teste é identificar todos os portadores de uma doença, o número de falsos negativos deve ser baixo, o que exige alta sensibilidade. Ou seja, as pessoas que têm a doença devem ter grande probabilidade de serem identificadas como tal pelo teste.\n",
        "\n",
        "Se o objetivo do teste é identificar com as pessoas que não têm a doença, o número de falsos positivos deve ser muito baixo, o que requer uma alta especificidade. Ou seja, as pessoas que não têm a doença devem ter grande probabilidade de serem excluídas do teste.\n",
        "\n",
        "#3- Considerações sobre o Naive Bayes\n",
        "\n",
        "Algumas vantagens do Naive Bayes sobre outros algoritmos de classificação são:\n",
        "\n",
        "- sua capacidade de lidar com um número extremamente grande de recursos;\n",
        "- sua relativa simplicidade, o ajuste de seus parâmetros raramente é necessário; e\n",
        "- os tempos de treinamento e previsão do modelo são muito rápidos para a quantidade de dados que ele pode manipular.\n",
        "\n",
        "#4 - Comparação Naive Bayes x Random Forest\n",
        "\n",
        "- O método Naive-bayes (0.9885) apresentou uma accurance ligeiramente melhor do que o\n",
        "Random Forest (0.9791); e\n",
        "- Além disso o Naive-bayes apresentou um número menor de Falso Negativo,\n",
        "ou seja, mensagens não spam classificadas como spam, \n",
        "num total de (11) contra (29) do Random Forest.\n",
        "Todos esses dados podem ser observados abaixo."
      ],
      "metadata": {
        "id": "c6ysOfAl3M1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDd096Rk3vkk",
        "outputId": "c4a2dc7f-ff73-4c9d-9400-beb3da36dd94"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "710     4mths half price Orange line rental & latest c...\n",
              "3740                           Did you stitch his trouser\n",
              "2711    Hope you enjoyed your new content. text stop t...\n",
              "3155    Not heard from U4 a while. Call 4 rude chat pr...\n",
              "3748    Ü neva tell me how i noe... I'm not at home in...\n",
              "                              ...                        \n",
              "905     We're all getting worried over here, derek and...\n",
              "5192    Oh oh... Den muz change plan liao... Go back h...\n",
              "3980    CERI U REBEL! SWEET DREAMZ ME LITTLE BUDDY!! C...\n",
              "235     Text & meet someone sexy today. U can find a d...\n",
              "5157                              K k:) sms chat with me.\n",
              "Name: sms_message, Length: 4179, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bju-BRqB4ouN",
        "outputId": "2d31af0d-cbc6-44a9-d535-89ad40cc3fed"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1078                         Yep, by the pretty sculpture\n",
              "4028        Yes, princess. Are you going to make me moan?\n",
              "958                            Welp apparently he retired\n",
              "4642                                              Havent.\n",
              "4674    I forgot 2 ask ü all smth.. There's a card on ...\n",
              "                              ...                        \n",
              "3207                                        At home also.\n",
              "4655                     Hope you are having a great day.\n",
              "1140    Message:some text missing* Sender:Name Missing...\n",
              "1793    WIN: We have a winner! Mr. T. Foley won an iPo...\n",
              "1710    U meet other fren dun wan meet me ah... Muz b ...\n",
              "Name: sms_message, Length: 1393, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hgundWv4r_I",
        "outputId": "e0285559-c9c7-462a-d436-896b616385c4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "710     1\n",
              "3740    0\n",
              "2711    1\n",
              "3155    1\n",
              "3748    0\n",
              "       ..\n",
              "905     0\n",
              "5192    0\n",
              "3980    0\n",
              "235     1\n",
              "5157    0\n",
              "Name: label, Length: 4179, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-Vt4NJv4vay",
        "outputId": "e22be24d-452c-4f52-a7c2-0284e3bf887f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1078    0\n",
              "4028    0\n",
              "958     0\n",
              "4642    0\n",
              "4674    0\n",
              "       ..\n",
              "3207    0\n",
              "4655    0\n",
              "1140    0\n",
              "1793    1\n",
              "1710    0\n",
              "Name: label, Length: 1393, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the CountVectorizer method\n",
        "count_vector = CountVectorizer()\n",
        "\n",
        "# Fit the training data and then return the matrix\n",
        "training_data = count_vector.fit_transform(X_train)\n",
        "\n",
        "# Transform testing data and return the matrix. Note we are not fitting the testing data into the CountVectorizer()\n",
        "testing_data = count_vector.transform(X_test)"
      ],
      "metadata": {
        "id": "07M1gnHU6KAR"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_jkYJ276MdO",
        "outputId": "07406e3f-a910-4424-b073-41e5d208643f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<4179x7456 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 55209 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A - Naive-Bayes"
      ],
      "metadata": {
        "id": "iycbHEJUAb2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "naive_bayes = MultinomialNB()\n",
        "naive_bayes.fit(training_data, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wyDPhEr5xpg",
        "outputId": "ea129cfb-746e-4c95-b164-ac3afb25451f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = naive_bayes.predict(testing_data)"
      ],
      "metadata": {
        "id": "ixRLVsiI6hqa"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
      ],
      "metadata": {
        "id": "yVEULbTg6yev"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_test, predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vShmaemb63mN",
        "outputId": "84bbba44-5a6f-4c82-cd45-d570bccd0175"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9885139985642498"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(y_test, predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-F1sAhX7I8n",
        "outputId": "9ab64380-073b-4ba9-ec63-0e7c9bd8f4c6"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1203,    5],\n",
              "       [  11,  174]])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from yellowbrick.classifier import ConfusionMatrix"
      ],
      "metadata": {
        "id": "g0J2xWbr7TFZ"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = ConfusionMatrix(naive_bayes)\n",
        "cm.fit(training_data, y_train)\n",
        "cm.score(testing_data, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "J_90w3NN7Ug7",
        "outputId": "2a144588-b108-4e34-dfb3-c606b2eb7b0e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9885139985642498"
            ]
          },
          "metadata": {},
          "execution_count": 48
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8PmqjeF8RtR",
        "outputId": "b4bfd5b0-ebc8-463b-e563-32ed68565325"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      1208\n",
            "           1       0.97      0.94      0.96       185\n",
            "\n",
            "    accuracy                           0.99      1393\n",
            "   macro avg       0.98      0.97      0.97      1393\n",
            "weighted avg       0.99      0.99      0.99      1393\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ],
      "metadata": {
        "id": "iNK3RxhhAzL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "gIhU4l86A100"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_forest = RandomForestClassifier(n_estimators=40, criterion='entropy', random_state = 0)\n",
        "random_forest.fit(training_data, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkfOIOszBBrG",
        "outputId": "58fec4a6-b4e7-4b33-d9bb-8fa2fc2eb189"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(criterion='entropy', n_estimators=40, random_state=0)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_predictions = random_forest.predict(testing_data)"
      ],
      "metadata": {
        "id": "wksAOVMVBTYh"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "accuracy_score(y_test, random_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O14ByZjtBjW2",
        "outputId": "3da08e0f-e76f-4548-f8e5-7e0bc09eb55c"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9791816223977028"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from yellowbrick.classifier import ConfusionMatrix\n",
        "cm = ConfusionMatrix(random_forest)\n",
        "cm.fit(training_data, y_train)\n",
        "cm.score(testing_data, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "-DUMYz11Bt2_",
        "outputId": "f249e17a-205c-4766-c2f9-ab5f66f2b0fa"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9791816223977028"
            ]
          },
          "metadata": {},
          "execution_count": 54
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, random_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZKWeNEqB-0m",
        "outputId": "3a12148d-67d1-467f-af84-ff63c86ae3f0"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99      1208\n",
            "           1       1.00      0.84      0.91       185\n",
            "\n",
            "    accuracy                           0.98      1393\n",
            "   macro avg       0.99      0.92      0.95      1393\n",
            "weighted avg       0.98      0.98      0.98      1393\n",
            "\n"
          ]
        }
      ]
    }
  ]
}